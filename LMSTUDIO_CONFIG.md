# LM Studioé…ç½®è¯´æ˜

## ğŸ¯ ä¸ºä»€ä¹ˆé€‰æ‹©LM Studioï¼Ÿ

ç›¸æ¯”Ollamaï¼ŒLM Studioæœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
- âœ… **æ›´å¥½çš„æ€§èƒ½é‡Šæ”¾**ï¼šGPUåˆ©ç”¨ç‡æ›´é«˜
- âœ… **OpenAIå…¼å®¹API**ï¼šæ ‡å‡†åŒ–æ¥å£ï¼Œæ˜“äºé›†æˆ
- âœ… **å›¾å½¢ç•Œé¢**ï¼šå¯è§†åŒ–ç®¡ç†æ¨¡å‹å’Œé…ç½®
- âœ… **çµæ´»çš„å‚æ•°è°ƒæ•´**ï¼šå®æ—¶è°ƒæ•´æ¸©åº¦ã€top-pç­‰å‚æ•°
- âœ… **æ›´ç¨³å®šçš„æ¨ç†**ï¼šèµ„æºç®¡ç†æ›´ä¼˜ç§€

---

## ğŸ“¥ å®‰è£…å’Œé…ç½®

### 1. ä¸‹è½½LM Studio

è®¿é—® [https://lmstudio.ai/](https://lmstudio.ai/) ä¸‹è½½å¯¹åº”ç³»ç»Ÿçš„ç‰ˆæœ¬ã€‚

### 2. ä¸‹è½½æ¨¡å‹

åœ¨LM Studioä¸­æœç´¢å¹¶ä¸‹è½½æ¨¡å‹ï¼Œæ¨èï¼š
- **Qwen2.5-7B-Instruct-GGUF**ï¼ˆä¸­æ–‡æ€§èƒ½ä¼˜ç§€ï¼‰
- **Llama-3.1-8B-Instruct-GGUF**ï¼ˆè‹±æ–‡æ€§èƒ½ä¼˜ç§€ï¼‰
- **Mistral-7B-Instruct-GGUF**ï¼ˆå¹³è¡¡é€‰æ‹©ï¼‰

### 3. å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨

1. åœ¨LM Studioä¸­ç‚¹å‡»"Local Server"æ ‡ç­¾
2. é€‰æ‹©å·²ä¸‹è½½çš„æ¨¡å‹
3. ç‚¹å‡»"Start Server"
4. é»˜è®¤è¿è¡Œåœ¨ `http://localhost:1234`

---

## âš™ï¸ ç³»ç»Ÿé…ç½®

### config.yaml é…ç½®

```yaml
model_routing:
  default: "lmstudio"
  
  lmstudio:
    base_url: "http://localhost:1234/v1"
    model: "local-model"  # å¯ä»¥æ˜¯ä»»æ„åç§°ï¼ŒLM Studioä¼šä½¿ç”¨å½“å‰åŠ è½½çš„æ¨¡å‹
  
  silicon:
    api_key: "your_api_key_here"
    model: "Qwen/Qwen2.5-72B-Instruct"
  
  routing_rules:
    local_nodes:
      - "main_body"
      - "literature_review"
    online_nodes:
      - "introduction"
      - "conclusion"
      - "abstract"
      - "expert_review"
```

---

## ğŸ”§ APIè°ƒç”¨ç¤ºä¾‹

### ç›´æ¥æµ‹è¯•LM Studio API

```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "local-model",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å­¦æœ¯å†™ä½œä¸“å®¶"},
      {"role": "user", "content": "è¯·ç®€è¦ä»‹ç»äººå·¥æ™ºèƒ½ä¼¦ç†"}
    ],
    "temperature": 0.7,
    "max_tokens": 500
  }'
```

### Pythonè°ƒç”¨ç¤ºä¾‹

```python
import requests

response = requests.post(
    "http://localhost:1234/v1/chat/completions",
    json={
        "model": "local-model",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å­¦æœ¯å†™ä½œä¸“å®¶"},
            {"role": "user", "content": "è¯·ç®€è¦ä»‹ç»äººå·¥æ™ºèƒ½ä¼¦ç†"}
        ],
        "temperature": 0.7,
        "max_tokens": 500
    }
)

result = response.json()
print(result['choices'][0]['message']['content'])
```

---

## ğŸ›ï¸ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. GPUè®¾ç½®
- ç¡®ä¿LM Studioä½¿ç”¨GPUåŠ é€Ÿ
- åœ¨è®¾ç½®ä¸­æŸ¥çœ‹GPUåˆ©ç”¨ç‡
- æ ¹æ®æ˜¾å­˜å¤§å°é€‰æ‹©æ¨¡å‹é‡åŒ–ç‰ˆæœ¬ï¼ˆQ4/Q5/Q8ï¼‰

### 2. ä¸Šä¸‹æ–‡é•¿åº¦
- æ ¹æ®æ˜¾å­˜è°ƒæ•´ä¸Šä¸‹æ–‡çª—å£å¤§å°
- å»ºè®®ï¼š8GBæ˜¾å­˜ â†’ 4096 tokensï¼Œ16GBæ˜¾å­˜ â†’ 8192 tokens

### 3. å¹¶å‘è®¾ç½®
- LM Studioæ”¯æŒå¹¶å‘è¯·æ±‚
- ä½†å»ºè®®è®ºæ–‡ç”Ÿæˆæ—¶ä½¿ç”¨å•çº¿ç¨‹ï¼ˆç¡®ä¿è´¨é‡ï¼‰

### 4. æ¸©åº¦å‚æ•°
- å­¦æœ¯å†™ä½œæ¨èï¼š0.7
- éœ€è¦æ›´ä¿å®ˆçš„è¾“å‡ºï¼š0.5
- éœ€è¦æ›´å¤šåˆ›æ„ï¼š0.8-0.9

---

## ğŸ”„ ä»Ollamaè¿ç§»

### å·²å®Œæˆçš„ä¿®æ”¹

1. **core/model_router.py**
   - æ›¿æ¢ `OllamaClient` ä¸º `LMStudioClient`
   - ä½¿ç”¨OpenAIå…¼å®¹APIæ ¼å¼

2. **config.yaml**
   - å°† `ollama` é…ç½®æ›¿æ¢ä¸º `lmstudio`
   - æ›´æ–°é»˜è®¤æ¨¡å¼ä¸º `lmstudio`

3. **README.md**
   - æ›´æ–°å¿«é€Ÿå¼€å§‹æŒ‡å—
   - æ·»åŠ LM Studioå®‰è£…è¯´æ˜

### æ— éœ€ä¿®æ”¹çš„éƒ¨åˆ†
- æ‰€æœ‰å…¶ä»–æ ¸å¿ƒæ¨¡å—ï¼ˆæ–‡çŒ®ç®¡ç†ã€å¼•ç”¨ç³»ç»Ÿã€å®¡ç¨¿ç³»ç»Ÿç­‰ï¼‰
- é…ç½®æ–‡ä»¶ç»“æ„å’Œè·¯ç”±è§„åˆ™
- Web APIæ¥å£

---

## â“ å¸¸è§é—®é¢˜

**Q: LM StudioæœåŠ¡å¯åŠ¨å¤±è´¥ï¼Ÿ**
A: æ£€æŸ¥ç«¯å£1234æ˜¯å¦è¢«å ç”¨ï¼Œæˆ–åœ¨LM Studioè®¾ç½®ä¸­æ›´æ”¹ç«¯å£

**Q: æ¨¡å‹æ¨ç†é€Ÿåº¦æ…¢ï¼Ÿ**
A: 
- æ£€æŸ¥æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
- å°è¯•æ›´å°çš„æ¨¡å‹æˆ–æ›´ä½çš„é‡åŒ–ç‰ˆæœ¬ï¼ˆQ4ï¼‰
- å‡å°‘max_tokenså‚æ•°

**Q: éœ€è¦æ›´æ”¹ç«¯å£å·ï¼Ÿ**
A: åœ¨LM Studioè®¾ç½®ä¸­æ›´æ”¹ç«¯å£ï¼ŒåŒæ—¶æ›´æ–° `config.yaml` ä¸­çš„ `base_url`

**Q: æ”¯æŒå¤šæ¨¡å‹åˆ‡æ¢å—ï¼Ÿ**
A: LM Studioä¸€æ¬¡åªèƒ½åŠ è½½ä¸€ä¸ªæ¨¡å‹ï¼Œåˆ‡æ¢éœ€è¦åœ¨ç•Œé¢ä¸­æ‰‹åŠ¨æ“ä½œ

---

**æ›´æ–°æ—¶é—´**ï¼š2025-12-20 19:20
**çŠ¶æ€**ï¼šå·²å®Œæˆä»Ollamaåˆ°LM Studioçš„è¿ç§»
